{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing in TensorFlow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2ops7oLGI-x",
        "colab_type": "code",
        "outputId": "c0a55075-dbc4-4a38-c3e9-cdb512ef217e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# !pip install tensorflow==2.0.0-beta0\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-beta0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQHLCc-q_XHH",
        "colab_type": "code",
        "outputId": "79de7ec3-b2e7-4e75-a1db-7c76598afdf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = ['I love My dog',\n",
        "             'I love you',\n",
        "             'l love my Cat',\n",
        "             'You all love my dog!!']\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'you': 5, 'l': 6, 'cat': 7, 'all': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky7S6i3t_dUX",
        "colab_type": "code",
        "outputId": "2938997c-f5ae-4245-9b07-7614b94b153a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = ['I love My dog',\n",
        "             'I love you',\n",
        "             'l love my Cat',\n",
        "             'You all love my dog!!']\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(word_index)\n",
        "print(sequences)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'you': 5, 'l': 6, 'cat': 7, 'all': 8}\n",
            "[[3, 1, 2, 4], [3, 1, 5], [6, 1, 2, 7], [5, 8, 1, 2, 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOvbc6rBrjRS",
        "colab_type": "text"
      },
      "source": [
        "## NLP ON IMDB Dataset Using Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYZjzxNknu08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is needed for the iterator over the data\n",
        "# But not necessary if you have TF 2.0 installed\n",
        "\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "# !pip install -q tensorflow-datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkk1KrjHbupT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTVNkv-Ibuso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_data, test_data = imdb['train'], imdb['test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvOqWTUrbuyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_data, test_data = imdb['train'], imdb['test']\n",
        "\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "\n",
        "testing_sentences = []\n",
        "testing_labels = []\n",
        "\n",
        "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
        "for s,l in train_data:\n",
        "  training_sentences.append(str(s.numpy()))\n",
        "  training_labels.append(l.numpy())\n",
        "  \n",
        "for s,l in test_data:\n",
        "  testing_sentences.append(str(s.numpy()))\n",
        "  testing_labels.append(l.numpy())\n",
        "  \n",
        "training_labels_final = np.array(training_labels)\n",
        "testing_labels_final = np.array(testing_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8b69Zorbuwi",
        "colab_type": "code",
        "outputId": "f76d295c-2cfa-4ccb-e56c-804e0a9ebff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(training_labels_final)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeTuqTZv_dhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDCFY5pE_dmn",
        "colab_type": "code",
        "outputId": "c93d5e58-01b5-42ca-d62f-15bcc6742295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_review(padded[1]))\n",
        "print(training_sentences[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b oh yeah jenna jameson did it again yeah baby this movie rocks it was one of the 1st movies i saw of her and i have to say i feel in love with her she was great in this move br br her performance was outstanding and what i liked the most was the scenery and the wardrobe it was amazing you can tell that they put a lot into the movie the girls cloth were amazing br br i hope this comment helps and u can buy the movie the storyline is awesome is very unique and i'm sure u are going to like it jenna amazed us once more and no wonder the movie won so many\n",
            "b\"Oh yeah! Jenna Jameson did it again! Yeah Baby! This movie rocks. It was one of the 1st movies i saw of her. And i have to say i feel in love with her, she was great in this move.<br /><br />Her performance was outstanding and what i liked the most was the scenery and the wardrobe it was amazing you can tell that they put a lot into the movie the girls cloth were amazing.<br /><br />I hope this comment helps and u can buy the movie, the storyline is awesome is very unique and i'm sure u are going to like it. Jenna amazed us once more and no wonder the movie won so many awards. Her make-up and wardrobe is very very sexy and the girls on girls scene is amazing. specially the one where she looks like an angel. It's a must see and i hope u share my interests\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlT2qwhuMZXj",
        "colab_type": "code",
        "outputId": "780b2f8d-cb78-43ca-a0db-4f7ee55354c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 120, 16)           160000    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1920)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 11526     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 171,533\n",
            "Trainable params: 171,533\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diOjv8QnKdM3",
        "colab_type": "code",
        "outputId": "e6f634e5-c5e2-4684-9c4b-17a0d94e5550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 120, 16)           160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 6)                 102       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 160,109\n",
            "Trainable params: 160,109\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndTdHBQtMZbf",
        "colab_type": "code",
        "outputId": "c75273c6-4304-4b13-a883-38ec4dd099d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "num_epochs = 10\n",
        "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 6s 258us/sample - loss: 0.5597 - accuracy: 0.7258 - val_loss: 0.4428 - val_accuracy: 0.8368\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 6s 254us/sample - loss: 0.3363 - accuracy: 0.8954 - val_loss: 0.4111 - val_accuracy: 0.8350\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 6s 253us/sample - loss: 0.1863 - accuracy: 0.9603 - val_loss: 0.4581 - val_accuracy: 0.8264\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 6s 227us/sample - loss: 0.1118 - accuracy: 0.9811 - val_loss: 0.4743 - val_accuracy: 0.8260\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 6s 224us/sample - loss: 0.0823 - accuracy: 0.9857 - val_loss: 0.5689 - val_accuracy: 0.8172\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 6s 230us/sample - loss: 0.0680 - accuracy: 0.9877 - val_loss: 0.5608 - val_accuracy: 0.8250\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 6s 243us/sample - loss: 0.0610 - accuracy: 0.9882 - val_loss: 0.5650 - val_accuracy: 0.8272\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 6s 253us/sample - loss: 0.0567 - accuracy: 0.9887 - val_loss: 0.6212 - val_accuracy: 0.8254\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 7s 271us/sample - loss: 0.0552 - accuracy: 0.9887 - val_loss: 0.6645 - val_accuracy: 0.8245\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 6s 243us/sample - loss: 0.0545 - accuracy: 0.9888 - val_loss: 0.6693 - val_accuracy: 0.8257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fae3e54b860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HoZIjQiK4SV",
        "colab_type": "code",
        "outputId": "3346ea46-4fbc-410c-e6ba-dddbbb26b1f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "num_epochs = 10\n",
        "model2.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 6s 259us/sample - loss: 0.5728 - accuracy: 0.7226 - val_loss: 0.4079 - val_accuracy: 0.8372\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 6s 244us/sample - loss: 0.3464 - accuracy: 0.8580 - val_loss: 0.3364 - val_accuracy: 0.8573\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 6s 229us/sample - loss: 0.2790 - accuracy: 0.8863 - val_loss: 0.3297 - val_accuracy: 0.8583\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 6s 224us/sample - loss: 0.2419 - accuracy: 0.9052 - val_loss: 0.3400 - val_accuracy: 0.8557\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 7s 264us/sample - loss: 0.2149 - accuracy: 0.9180 - val_loss: 0.3580 - val_accuracy: 0.8514\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 6s 240us/sample - loss: 0.1935 - accuracy: 0.9294 - val_loss: 0.3829 - val_accuracy: 0.8456\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 6s 245us/sample - loss: 0.1755 - accuracy: 0.9362 - val_loss: 0.4266 - val_accuracy: 0.8363\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 6s 222us/sample - loss: 0.1600 - accuracy: 0.9448 - val_loss: 0.4418 - val_accuracy: 0.8346\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 6s 251us/sample - loss: 0.1471 - accuracy: 0.9498 - val_loss: 0.4758 - val_accuracy: 0.8288\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 6s 233us/sample - loss: 0.1342 - accuracy: 0.9548 - val_loss: 0.5110 - val_accuracy: 0.8255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fae3d7446d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qSd6v7Q3_Qx",
        "colab_type": "code",
        "outputId": "f523f673-c4e7-4545-9812-a87b33402e77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKBMERhN3_UL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnIr58T13_YK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRxxoObdgndo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = \"I really think this is amazing. honest.\"\n",
        "sequence = tokenizer.texts_to_sequences(sentence)\n",
        "print(sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4PnUSzwQoFH",
        "colab_type": "text"
      },
      "source": [
        "## 2. NLP ON Sarcasm Dataset Using Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itZHrXbkgnm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}